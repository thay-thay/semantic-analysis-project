Timestamp,First_Name,Last_Name,Programming,Data_Analysis,ML_Projects,ML_Problem,NLP,Data_Pipeline,Sharing_Results,Git_Level,Presentation_Level,Reflection
2025-10-09 08:17:53,test,Test CSV,ok,ok,ok,ok,ok,ok,o,4,3,k
2025-10-09 08:23:45,Alex,Dupont,"I have 3 years of experience with Python, particularly with pandas, numpy, and scikit-learn. I'm comfortable with SQL for database queries and data extraction. I use Git daily for version control and collaborate with my team through GitHub. I understand OOP principles and write modular, reusable code. I also have some experience with R for statistical analysis.","My typical workflow starts with data cleaning - handling missing values, removing duplicates, and checking for outliers. Then I perform exploratory data analysis using pandas profiling and matplotlib. I create visualizations to understand distributions, correlations, and trends. I calculate descriptive statistics and use hypothesis testing when needed. I document my findings in Jupyter notebooks.","My typical workflow starts with data cleaning - handling missing values, removing duplicates, and checking for outliers. Then I perform exploratory data analysis using pandas profiling and matplotlib. I create visualizations to understand distributions, correlations, and trends. I calculate descriptive statistics and use hypothesis testing when needed. I document my findings in Jupyter notebooks.","For a churn prediction model, I would start by understanding the business context and defining the target variable. Then I'd engineer features like customer tenure, usage patterns, and payment history. I'd split the data into train/test sets, handle class imbalance, and test multiple algorithms. I'd evaluate using precision, recall, and AUC-ROC, then deploy the best model with monitoring.","Yes, I worked on a sentiment analysis project for customer reviews. I used NLTK for tokenization and preprocessing, removed stopwords, and applied TF-IDF vectorization. I experimented with BERT embeddings for better context understanding. I also built a named entity recognition system to extract product names from feedback using spaCy.","I built an ETL pipeline using Apache Airflow to process daily sales data. The pipeline extracts data from multiple sources, transforms it by cleaning and aggregating, then loads it into a PostgreSQL database. I scheduled it to run automatically every night and set up alerts for failures. I also optimized it for performance with parallel processing.","I create interactive dashboards using Plotly and Streamlit to visualize KPIs. For technical audiences, I prepare detailed Jupyter notebooks with code and analysis. For business stakeholders, I make PowerPoint presentations with clear insights and recommendations. I always explain the methodology, limitations, and actionable next steps.",4,4,"A strong Data Scientist combines technical skills with business understanding. They need solid programming and statistics knowledge, but also the ability to communicate complex findings simply. Curiosity, problem-solving mindset, and continuous learning are essential. They should understand when to use which algorithm and always validate their assumptions with data."
