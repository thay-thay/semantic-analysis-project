Timestamp,First_Name,Last_Name,Programming,Data_Analysis,ML_Projects,ML_Problem,NLP,Data_Pipeline,Sharing_Results,Git_Level,Presentation_Level,Reflection
2025-10-09 08:17:53,test,Test CSV,ok,ok,ok,ok,ok,ok,o,4,3,k
2025-10-09 08:23:45,Alex,Dupont,"I have 3 years of experience with Python, particularly with pandas, numpy, and scikit-learn. I'm comfortable with SQL for database queries and data extraction. I use Git daily for version control and collaborate with my team through GitHub. I understand OOP principles and write modular, reusable code. I also have some experience with R for statistical analysis.","My typical workflow starts with data cleaning - handling missing values, removing duplicates, and checking for outliers. Then I perform exploratory data analysis using pandas profiling and matplotlib. I create visualizations to understand distributions, correlations, and trends. I calculate descriptive statistics and use hypothesis testing when needed. I document my findings in Jupyter notebooks.","My typical workflow starts with data cleaning - handling missing values, removing duplicates, and checking for outliers. Then I perform exploratory data analysis using pandas profiling and matplotlib. I create visualizations to understand distributions, correlations, and trends. I calculate descriptive statistics and use hypothesis testing when needed. I document my findings in Jupyter notebooks.","For a churn prediction model, I would start by understanding the business context and defining the target variable. Then I'd engineer features like customer tenure, usage patterns, and payment history. I'd split the data into train/test sets, handle class imbalance, and test multiple algorithms. I'd evaluate using precision, recall, and AUC-ROC, then deploy the best model with monitoring.","Yes, I worked on a sentiment analysis project for customer reviews. I used NLTK for tokenization and preprocessing, removed stopwords, and applied TF-IDF vectorization. I experimented with BERT embeddings for better context understanding. I also built a named entity recognition system to extract product names from feedback using spaCy.","I built an ETL pipeline using Apache Airflow to process daily sales data. The pipeline extracts data from multiple sources, transforms it by cleaning and aggregating, then loads it into a PostgreSQL database. I scheduled it to run automatically every night and set up alerts for failures. I also optimized it for performance with parallel processing.","I create interactive dashboards using Plotly and Streamlit to visualize KPIs. For technical audiences, I prepare detailed Jupyter notebooks with code and analysis. For business stakeholders, I make PowerPoint presentations with clear insights and recommendations. I always explain the methodology, limitations, and actionable next steps.",4,4,"A strong Data Scientist combines technical skills with business understanding. They need solid programming and statistics knowledge, but also the ability to communicate complex findings simply. Curiosity, problem-solving mindset, and continuous learning are essential. They should understand when to use which algorithm and always validate their assumptions with data."
2025-10-10 08:49:12,Corentin,Coffre,"I specialize in Python with deep expertise in NLP libraries: transformers, spaCy, NLTK, Hugging Face. Also proficient in PyTorch for custom model architectures. Use SQL for data extraction.I specialize in Python with deep expertise in NLP libraries: transformers, spaCy, NLTK, Hugging Face. Also proficient in PyTorch for custom model architectures. Use SQL for data extraction.","For text data, I check corpus statistics, token distributions, vocabulary size. Analyze text length distributions, language detection, encoding issues. For tabular data, standard EDA with pandas and visualizations.For text data, I check corpus statistics, token distributions, vocabulary size. Analyze text length distributions, language detection, encoding issues. For tabular data, standard EDA with pandas and visualizations.","I have basic ML knowledge - logistic regression, random forests for classification. But my focus is on NLP-specific modeling with transformers and language models.I have basic ML knowledge - logistic regression, random forests for classification. But my focus is on NLP-specific modeling with transformers and language models.","I would approach it as a classification problem. Extract text features from customer interactions if available. Use embeddings to capture semantic meaning. Try logistic regression baseline, then gradient boosting. But this is not my core expertise.I would approach it as a classification problem. Extract text features from customer interactions if available. Use embeddings to capture semantic meaning. Try logistic regression baseline, then gradient boosting. But this is not my core expertise.","This is my core expertise. I have 5 years in NLP. Built multiple text classification systems with BERT, RoBERTa, and DistilBERT. Fine-tuned models for sentiment analysis, intent classification, and content moderation. Implemented named entity recognition with spaCy and transformer models. Created question-answering systems using T5 and BART. Built chatbots with GPT and dialogue transformers. Worked on machine translation and text summarization. Used tokenization, lemmatization, POS tagging extensively. Deployed models with FastAPI and Docker.This is my core expertise. I have 5 years in NLP. Built multiple text classification systems with BERT, RoBERTa, and DistilBERT. Fine-tuned models for sentiment analysis, intent classification, and content moderation. Implemented named entity recognition with spaCy and transformer models. Created question-answering systems using T5 and BART. Built chatbots with GPT and dialogue transformers. Worked on machine translation and text summarization. Used tokenization, lemmatization, POS tagging extensively. Deployed models with FastAPI and Docker.",Limited experience. I focus on model development. Data engineers handle pipeline infrastructure in my team.Limited experience. I focus on model development. Data engineers handle pipeline infrastructure in my team.,"I create model performance reports with precision, recall, F1-scores. Visualize confusion matrices and error analysis. Present to technical teams and explain model capabilities and limitations to product managers.I create model performance reports with precision, recall, F1-scores. Visualize confusion matrices and error analysis. Present to technical teams and explain model capabilities and limitations to product managers.",4,3,"Deep understanding of language models, transformers architecture, training techniques, and ability to fine-tune models for specific tasks."
